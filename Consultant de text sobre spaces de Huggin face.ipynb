{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd730d-bb8a-46c1-9fc1-1d9f7c790c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c16bbb-4f7a-4068-9839-55969f036d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from gradio_client import Client\n",
    "\n",
    "# Defineix la teva clau d'API de Hugging Face\n",
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff20e70d-611c-4fd1-bc5c-3a622c8e1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega els documents des d'un directori de fitxers JSON\n",
    "def load_json_documents(directory_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = json.load(file)\n",
    "                text = \"\"\n",
    "                for section in content.get('content', {}).values():\n",
    "                    if isinstance(section, list):\n",
    "                        text += \"\\n\".join(section)\n",
    "                    else:\n",
    "                        text += section\n",
    "                documents.append(Document(page_content=text, metadata={\"title\": content.get(\"title\"), \"url\": content.get(\"url\")}))\n",
    "    return documents\n",
    "\n",
    "# Vectoritza els documents\n",
    "def vectorize_documents(documents):\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Crea un prompt basat en els vectors i la pregunta\n",
    "def create_prompt_from_vectors(vectorstore, question):\n",
    "    docs = vectorstore.similarity_search(question, k=5)\n",
    "    combined_docs = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"{combined_docs}\\n\\nPregunta: {question}\\nResposta:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20404524-2c13-46d8-96a2-e6751c9f100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Carrega i vectoritza els documents\u001b[39;00m\n\u001b[0;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_json_documents(document_directory)\n\u001b[1;32m----> 6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m vectorize_documents(documents)\n",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mvectorize_documents\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvectorize_documents\u001b[39m(documents):\n\u001b[0;32m     19\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings()\n\u001b[1;32m---> 20\u001b[0m     vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(documents, embeddings)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1058\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m   1056\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m   1057\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m         texts,\n\u001b[0;32m    933\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:105\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    103\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m    106\u001b[0m         texts, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:565\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 565\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m all_embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[0;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Ruta al directori dels teus documents JSON\n",
    "document_directory = \"C:\\\\Users\\\\Naudor\\\\prova_chatgpt\\\\documents\\\\Telefonica\"\n",
    "\n",
    "# Carrega i vectoritza els documents\n",
    "documents = load_json_documents(document_directory)\n",
    "vectorstore = vectorize_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7fc234-a722-40c0-84c1-d956e8b4b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://vilarin-llama-3-1-8b-instruct.hf.space ✔\n",
      "M'he decantat per un teclat mecànic totalment innovador. Alguns dels beneficis del Moonlander MK1 són:\n",
      "\n",
      "*   Dividit en dues meitats independents.\n",
      "*   Utilitza capes per assignarfuncions diferents les tecles, facilitant la personalització \n",
      "*   Incorpora la tecnologia \"ortholineal\" en les tecles (ordenades en columnes verticalments). \n",
      "*   Inclou il.luminació RGB amb possibilitats de personalització. \n",
      "\n",
      "*   Teclat ajustable en altura en les zones dels polzes (inclined) a través del moduli.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta que vols fer\n",
    "question = \"Quina opinió tinc de Moonlander MK1?\"\n",
    "\n",
    "# Genera el prompt a partir dels vectors i envia la sol·licitud a l'API\n",
    "prompt = create_prompt_from_vectors(vectorstore, question)\n",
    "\n",
    "client = Client(\"vilarin/Llama-3.1-8B-Instruct\")\n",
    "result = client.predict(\n",
    "\t\tmessage=prompt,\n",
    "\t\tsystem_prompt=\"Constesta sempre en català. Estas contestant en base a articles que he escrit jo\",\n",
    "\t\ttemperature=0.8,\n",
    "\t\tmax_new_tokens=4096,\n",
    "\t\ttop_p=1,\n",
    "\t\ttop_k=20,\n",
    "\tpenalty=1.2,\n",
    "\tapi_name=\"/chat\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124545af-4e6b-465a-a42c-3e229ab78cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
